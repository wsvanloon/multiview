% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/StaPLR.R
\name{StaPLR}
\alias{StaPLR}
\title{Stacked Penalized Logistic Regression}
\usage{
StaPLR(x, y, view, view.names = NULL, correct.for = NULL, alpha1 = 0,
  alpha2 = 1, nfolds = 5, seed = NULL, std.base = FALSE,
  std.meta = FALSE, ll1 = -Inf, ul1 = Inf, ll2 = 0, ul2 = Inf,
  cvloss = "deviance", metadat = "response", cvlambda = "lambda.min",
  cvparallel = FALSE, lambda.ratio = 0.01, penalty.weights = NULL,
  parallel = FALSE, skip.fdev = FALSE, skip.version = FALSE,
  progress = TRUE)
}
\arguments{
\item{x}{input matrix of dimension nobs x nvars}

\item{y}{outcome vector of length nobs}

\item{view}{a vector of length nvars, where each entry is an integer describing to which view each feature corresponds.}

\item{view.names}{(optional) a character vector of length nviews specifying a name for each view.}

\item{correct.for}{(optional) a matrix with nrow = nobs, where each column is a feature which should be included directly into the meta.learner. By default these features are not penalized (see penalty.weights) and appear at the top of the coefficient list.}

\item{alpha1}{(base) alpha parameter for glmnet: lasso(1) / ridge(0)}

\item{alpha2}{(meta) alpha parameter for glmnet: lasso(1) / ridge(0)}

\item{nfolds}{number of folds to use for all cross-validation.}

\item{seed}{(optional) numeric value specifying the seed. Setting the seed this way ensures the results are reproducable even when the computations are performed in parallel.}

\item{std.base}{should features be standardized at the base level?}

\item{std.meta}{should cross-validated predictions be standardized at the meta level?}

\item{ll1}{lower limit(s) for each coefficient at the base-level. Defaults to -Inf.}

\item{ul1}{upper limit(s) for each coefficient at the base-level. Defaults to Inf.}

\item{ll2}{lower limit(s) for each coefficient at the meta-level. Defaults to 0 (non-negativity constraints). Does not apply to correct.for features.}

\item{ul2}{upper limit(s) for each coefficient at the meta-level. Defaults to Inf. Does not apply to correct.for features.}

\item{cvloss}{loss to use for cross-validation.}

\item{metadat}{which attribute of the base learners should be used as input for the meta learner?}

\item{cvlambda}{value of lambda at which cross-validated predictions are made.}

\item{cvparallel}{whether to use 'foreach' to fit each CV fold (DO NOT USE, USE OPTION parallel INSTEAD).}

\item{lambda.ratio}{the ratio between the largest and smallest lambda value.}

\item{penalty.weights}{(optional) a vector of length nviews, containing different penalty factors for the meta-learner. Defaults to rep(1,nviews). The penalty factor is set to 0 for correct.for features.}

\item{parallel}{whether to use foreach to fit the base-learners and obtain the cross-validated predictions in parallel. Executes sequentially unless a parallel backend is registered beforehand.}

\item{skip.fdev}{whether to skip checking if the fdev parameter is set to zero.}

\item{skip.version}{whether to skip checking the version of the glmnet package.}

\item{progress}{whether to show a progress bar (only supported when parallel = FALSE).}
}
\value{
TBA.
}
\description{
Fit a two-level stacked penalized logistic regression model with a single base-learner and a single meta-learner.
}
\examples{
set.seed(012)
n <- 1000
cors <- seq(0.1,0.7,0.1)
X <- matrix(NA, nrow=n, ncol=length(cors)+1)
X[,1] <- rnorm(n)

for(i in 1:length(cors)){
  X[,i+1] <- X[,1]*cors[i] + rnorm(n, 0, sqrt(1-cors[i]^2))
}

beta <- c(1,0,0,0,0,0,0,0)
eta <- X \%*\% beta
p <- exp(eta)/(1+exp(eta))
y <- rbinom(n, 1, p)
view_index <- rep(1:(ncol(X)/2), each=2)

fit <- StaPLR(X, y, view_index)
coef(fit$meta, s="lambda.min")

new_X <- matrix(rnorm(16), nrow=2)
predict(fit, new_X)
}
\author{
Wouter van Loon <w.s.van.loon@fsw.leidenuniv.nl>
}
\keyword{TBA}
